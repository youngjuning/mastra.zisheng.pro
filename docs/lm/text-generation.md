---
order: 1
title: 文本生成模型
description: '大语言模型是专门用于理解和生成人类语言的神经网络模型，它的输入和输出仅限于文本（token 序列）。 '
keywords: [大语言模型, 文本生成模型, 大模型, AI, Agents]
nav:
  title: 大模型
  order: 2
toc: content
---

## 概述

文本生成模型能够基于输入的提示词（Prompt）创作出逻辑清晰、连贯的文本。

文本生成模型所需的输入可以是简单的关键词、一句话概述或是更复杂的指令和上下文信息。模型通过分析海量数据学习语言模式，广泛应用于：

- 内容创作：生成新闻报道、商品介绍及短视频脚本。
- 客户服务：驱动聊天机器人提供全天候支持，解答常见问题。
- 文本翻译：实现跨语言的快速精准转换。
- 摘要生成：提炼长文、报告及邮件的核心内容。
- 法律文档编写：生成合同模板、法律意见书的基础框架。

## 核心概念

文本生成模型（Text Generation Model）的输入为提示词（Prompt），它由一个或多个消息（Message）对象构成。每条消息由角色（Role）和内容（Content）组成，具体为：

- **系统消息（System Message）**：设定模型扮演的角色或遵循的指令。若不指定，默认为"You are a helpful assistant"。
- **用户消息（User Message）**：用户向模型提出的问题或输入的指令。
- **助手消息（Assistant Message）**：模型的回复内容。

调用模型时，需构造一个由上述消息对象构成的数组`messages`。一个典型的请求通常由一条定义行为准则的 `system` 消息和一条用户指令的 `user` 消息组成。

:::info
`system`消息是可选的，但建议使用它来设定模型的角色和行为准则，以获得更稳定、一致的输出。
:::

```json
[
  {"role": "system", "content": "你是一个有帮助的助手，需要提供精准、高效且富有洞察力的回应，随时准备协助用户处理各种任务与问题。"},
  {"role": "user", "content": "你是谁？"}
]
```

输出的响应对象中会包含

## 大语言模型

### 定义

大语言模型（Large Language Model）是一种参数量巨大（通常 >1B）、在海量无标注文本上预训练、具备通用语言理解与生成能力的神经网络模型。这是一个结构+规模+训练方式导向的类别。

### 和文本生成模型的区别

大语言模型是一种特殊的、强大的文本生成模型，但并非所有文本生成模型都是大语言模型。

**关键区别详解**

| 维度                       | 文本生成模型（广义）                                         | 大语言模型（LLM）                                    |
| -------------------------- | ------------------------------------------------------------ | ---------------------------------------------------- |
| **规模**                   | 可大可小（几万到几十亿参数）                                 | **极大**（通常 7B ~ 数万亿参数）                     |
| **训练数据**               | 特定任务数据（如新闻摘要、对话日志）                         | **全网海量通用文本**（网页、书籍、代码等）           |
| **训练方式**               | 有监督微调为主                                               | **自监督预训练 + 指令微调 + RLHF**                   |
| **通用性**                 | 通常**专用**（如只做翻译、只写邮件）                         | **通用**（能聊天、编程、推理、创作）                 |
| **是否需要提示**（Prompt） | 不一定（有些端到端）                                         | **高度依赖提示工程**                                 |
| **例子**                   | - Seq2Seq (2014) - LSTM 语言模型 - BART（摘要） - T5（多任务） | - GPT 系列 - Llama 系列 - Qwen / Baichuan / DeepSeek |

**历史演进视角**

1. **2014–2017：文本生成 = 小模型时代**
   - 使用 RNN、LSTM 生成短文本（如机器翻译、标题生成）
   - 模型小，只能完成**单一任务**

2. **2018–2020：Transformer 崛起**
   - BERT（理解）、GPT-2（生成）出现
   - 开始有“预训练+微调”范式，但还不叫“大模型”

3. **2020–至今：大语言模型时代**
   - GPT-3（175B 参数）引爆“大模型”概念
   - **一个模型通吃所有文本任务** → 成为“文本生成”的代名词
   - 但技术上，它只是文本生成模型的**一个高级形态**

> 💡 正因为 LLM 太成功，很多人**误以为“文本生成 = LLM”**，就像“手机 = iPhone”一样是以偏概全。
