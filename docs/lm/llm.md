---
order: 2
title: 大语言模型
description: '大语言模型（Large Language Model）是一种参数量巨大（通常 >1B）、在海量无标注文本上预训练、具备通用语言理解与生成能力的神经网络模型。'
keywords: [大语言模型, 文本生成模型, 大模型, AI, Agents]
toc: content
---

## 定义

大语言模型（Large Language Model）是一种参数量巨大（通常 >1B）、在海量无标注文本上预训练、具备通用语言理解与生成能力的神经网络模型。这是一个结构+规模+训练方式导向的类别。

## 和文本生成模型的区别

大语言模型是一种特殊的、强大的文本生成模型，但并非所有文本生成模型都是大语言模型。

**关键区别详解**

| 维度                       | 文本生成模型（广义）                                         | 大语言模型（LLM）                                    |
| -------------------------- | ------------------------------------------------------------ | ---------------------------------------------------- |
| **规模**                   | 可大可小（几万到几十亿参数）                                 | **极大**（通常 7B ~ 数万亿参数）                     |
| **训练数据**               | 特定任务数据（如新闻摘要、对话日志）                         | **全网海量通用文本**（网页、书籍、代码等）           |
| **训练方式**               | 有监督微调为主                                               | **自监督预训练 + 指令微调 + RLHF**                   |
| **通用性**                 | 通常**专用**（如只做翻译、只写邮件）                         | **通用**（能聊天、编程、推理、创作）                 |
| **是否需要提示**（Prompt） | 不一定（有些端到端）                                         | **高度依赖提示工程**                                 |
| **例子**                   | - Seq2Seq (2014) - LSTM 语言模型 - BART（摘要） - T5（多任务） | - GPT 系列 - Llama 系列 - Qwen / Baichuan / DeepSeek |

**历史演进视角**

1. **2014–2017：文本生成 = 小模型时代**
   - 使用 RNN、LSTM 生成短文本（如机器翻译、标题生成）
   - 模型小，只能完成**单一任务**

2. **2018–2020：Transformer 崛起**
   - BERT（理解）、GPT-2（生成）出现
   - 开始有“预训练+微调”范式，但还不叫“大模型”

3. **2020–至今：大语言模型时代**
   - GPT-3（175B 参数）引爆“大模型”概念
   - **一个模型通吃所有文本任务** → 成为“文本生成”的代名词
   - 但技术上，它只是文本生成模型的**一个高级形态**

> 💡 正因为 LLM 太成功，很多人**误以为“文本生成 = LLM”**，就像“手机 = iPhone”一样是以偏概全。
